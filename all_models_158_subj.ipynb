{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_math_verbal(conn_mat_filename: str, general_scores_filename: str, math_scores_filename: str, verbal_scores_filename: str):\n",
    "    Glasser_conn_mat = np.load(conn_mat_filename)\n",
    "\n",
    "    # get upper triangle indices, without diagonal 0 (k = 1 starts from the k' diagonal)\n",
    "    indices = np.triu_indices(360, k=1)\n",
    "\n",
    "    # create X: rows for subjects, and flattened upper triangle mat for each subject\n",
    "    X = []\n",
    "    for i in range(Glasser_conn_mat.shape[2]):\n",
    "        X.append(Glasser_conn_mat[:, :, i][indices])\n",
    "    # convert X to data frame for pipeline parameters\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # read scores\n",
    "    y = pd.read_csv(general_scores_filename, header=None).to_numpy()\n",
    "    y_math = pd.read_csv(math_scores_filename, header=None).to_numpy()\n",
    "    y_verbal = pd.read_csv(verbal_scores_filename, header=None).to_numpy()\n",
    "\n",
    "    return X, y, y_math, y_verbal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, y_math, y_verbal= get_X_y_math_verbal(\n",
    "    conn_mat_filename=\"data/Glasser_conn_mat_158_subj.npy\",\n",
    "    general_scores_filename=\"data/general_scores_158_subj.csv\",\n",
    "    math_scores_filename=\"data/math_scores_158_subj.csv\",\n",
    "    verbal_scores_filename=\"data/verbal_scores_158_subj.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>64610</th>\n",
       "      <th>64611</th>\n",
       "      <th>64612</th>\n",
       "      <th>64613</th>\n",
       "      <th>64614</th>\n",
       "      <th>64615</th>\n",
       "      <th>64616</th>\n",
       "      <th>64617</th>\n",
       "      <th>64618</th>\n",
       "      <th>64619</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.256043</td>\n",
       "      <td>0.543582</td>\n",
       "      <td>0.834341</td>\n",
       "      <td>0.737259</td>\n",
       "      <td>0.611785</td>\n",
       "      <td>0.551843</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.506335</td>\n",
       "      <td>0.347711</td>\n",
       "      <td>0.313753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048507</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>0.108180</td>\n",
       "      <td>0.056225</td>\n",
       "      <td>0.042079</td>\n",
       "      <td>0.084301</td>\n",
       "      <td>0.041814</td>\n",
       "      <td>0.307135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.483221</td>\n",
       "      <td>0.612067</td>\n",
       "      <td>0.912712</td>\n",
       "      <td>0.845826</td>\n",
       "      <td>0.808821</td>\n",
       "      <td>0.462722</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.297347</td>\n",
       "      <td>0.760023</td>\n",
       "      <td>0.656678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455484</td>\n",
       "      <td>0.171413</td>\n",
       "      <td>0.093261</td>\n",
       "      <td>0.365236</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.193829</td>\n",
       "      <td>0.247808</td>\n",
       "      <td>0.266242</td>\n",
       "      <td>0.295980</td>\n",
       "      <td>0.532271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126272</td>\n",
       "      <td>0.541725</td>\n",
       "      <td>0.894314</td>\n",
       "      <td>0.881399</td>\n",
       "      <td>0.768822</td>\n",
       "      <td>0.541432</td>\n",
       "      <td>0.282807</td>\n",
       "      <td>0.175312</td>\n",
       "      <td>0.360123</td>\n",
       "      <td>0.392476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291799</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>0.026356</td>\n",
       "      <td>0.229688</td>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.049035</td>\n",
       "      <td>0.071814</td>\n",
       "      <td>0.219577</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.439387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528645</td>\n",
       "      <td>0.548732</td>\n",
       "      <td>0.897949</td>\n",
       "      <td>0.874365</td>\n",
       "      <td>0.722720</td>\n",
       "      <td>0.452277</td>\n",
       "      <td>0.589948</td>\n",
       "      <td>0.519460</td>\n",
       "      <td>0.393255</td>\n",
       "      <td>0.331269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277162</td>\n",
       "      <td>0.061410</td>\n",
       "      <td>0.055927</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.063941</td>\n",
       "      <td>0.240621</td>\n",
       "      <td>0.062928</td>\n",
       "      <td>0.136225</td>\n",
       "      <td>0.082756</td>\n",
       "      <td>0.281293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.408504</td>\n",
       "      <td>0.533769</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.885054</td>\n",
       "      <td>0.722196</td>\n",
       "      <td>0.653343</td>\n",
       "      <td>0.433824</td>\n",
       "      <td>-0.006783</td>\n",
       "      <td>0.548261</td>\n",
       "      <td>0.341645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>-0.084169</td>\n",
       "      <td>-0.293965</td>\n",
       "      <td>-0.067129</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>-0.066016</td>\n",
       "      <td>0.092298</td>\n",
       "      <td>0.061617</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>0.311022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.497834</td>\n",
       "      <td>0.592031</td>\n",
       "      <td>0.945841</td>\n",
       "      <td>0.886041</td>\n",
       "      <td>0.737326</td>\n",
       "      <td>0.665670</td>\n",
       "      <td>0.324835</td>\n",
       "      <td>0.187993</td>\n",
       "      <td>0.534930</td>\n",
       "      <td>0.465906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435125</td>\n",
       "      <td>0.082796</td>\n",
       "      <td>0.125401</td>\n",
       "      <td>0.225401</td>\n",
       "      <td>0.045384</td>\n",
       "      <td>0.137809</td>\n",
       "      <td>0.239056</td>\n",
       "      <td>0.178493</td>\n",
       "      <td>0.124486</td>\n",
       "      <td>0.334965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.274340</td>\n",
       "      <td>0.601339</td>\n",
       "      <td>0.899841</td>\n",
       "      <td>0.832969</td>\n",
       "      <td>0.679898</td>\n",
       "      <td>0.506748</td>\n",
       "      <td>0.209305</td>\n",
       "      <td>0.273583</td>\n",
       "      <td>0.391672</td>\n",
       "      <td>0.145301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274637</td>\n",
       "      <td>0.080526</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.204128</td>\n",
       "      <td>0.080962</td>\n",
       "      <td>0.033422</td>\n",
       "      <td>0.177067</td>\n",
       "      <td>0.244266</td>\n",
       "      <td>0.238470</td>\n",
       "      <td>0.499744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.649097</td>\n",
       "      <td>0.548860</td>\n",
       "      <td>0.792040</td>\n",
       "      <td>0.734714</td>\n",
       "      <td>0.501993</td>\n",
       "      <td>0.564024</td>\n",
       "      <td>0.376806</td>\n",
       "      <td>0.250711</td>\n",
       "      <td>0.529989</td>\n",
       "      <td>0.315082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481008</td>\n",
       "      <td>0.202375</td>\n",
       "      <td>-0.087569</td>\n",
       "      <td>-0.172030</td>\n",
       "      <td>0.112162</td>\n",
       "      <td>0.011920</td>\n",
       "      <td>-0.065025</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>0.059129</td>\n",
       "      <td>0.414311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.457167</td>\n",
       "      <td>0.697228</td>\n",
       "      <td>0.949361</td>\n",
       "      <td>0.923404</td>\n",
       "      <td>0.861765</td>\n",
       "      <td>0.720488</td>\n",
       "      <td>0.334928</td>\n",
       "      <td>0.192592</td>\n",
       "      <td>0.507035</td>\n",
       "      <td>0.459623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261736</td>\n",
       "      <td>0.200122</td>\n",
       "      <td>0.139826</td>\n",
       "      <td>0.057603</td>\n",
       "      <td>0.277383</td>\n",
       "      <td>0.374788</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.280210</td>\n",
       "      <td>0.196362</td>\n",
       "      <td>0.292409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.206464</td>\n",
       "      <td>0.591203</td>\n",
       "      <td>0.831999</td>\n",
       "      <td>0.796237</td>\n",
       "      <td>0.584414</td>\n",
       "      <td>0.506734</td>\n",
       "      <td>0.296378</td>\n",
       "      <td>0.323051</td>\n",
       "      <td>0.333396</td>\n",
       "      <td>0.076793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570339</td>\n",
       "      <td>0.149866</td>\n",
       "      <td>0.088419</td>\n",
       "      <td>0.147588</td>\n",
       "      <td>0.335105</td>\n",
       "      <td>0.178705</td>\n",
       "      <td>0.190511</td>\n",
       "      <td>0.100918</td>\n",
       "      <td>0.140988</td>\n",
       "      <td>0.321525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 64620 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0    0.256043  0.543582  0.834341  0.737259  0.611785  0.551843  0.268657   \n",
       "1    0.483221  0.612067  0.912712  0.845826  0.808821  0.462722  0.192100   \n",
       "2    0.126272  0.541725  0.894314  0.881399  0.768822  0.541432  0.282807   \n",
       "3    0.528645  0.548732  0.897949  0.874365  0.722720  0.452277  0.589948   \n",
       "4    0.408504  0.533769  0.918164  0.885054  0.722196  0.653343  0.433824   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "153  0.497834  0.592031  0.945841  0.886041  0.737326  0.665670  0.324835   \n",
       "154  0.274340  0.601339  0.899841  0.832969  0.679898  0.506748  0.209305   \n",
       "155  0.649097  0.548860  0.792040  0.734714  0.501993  0.564024  0.376806   \n",
       "156  0.457167  0.697228  0.949361  0.923404  0.861765  0.720488  0.334928   \n",
       "157  0.206464  0.591203  0.831999  0.796237  0.584414  0.506734  0.296378   \n",
       "\n",
       "        7         8         9      ...     64610     64611     64612  \\\n",
       "0    0.506335  0.347711  0.313753  ...  0.048507 -0.001441  0.037499   \n",
       "1    0.297347  0.760023  0.656678  ...  0.455484  0.171413  0.093261   \n",
       "2    0.175312  0.360123  0.392476  ...  0.291799  0.033146  0.026356   \n",
       "3    0.519460  0.393255  0.331269  ...  0.277162  0.061410  0.055927   \n",
       "4   -0.006783  0.548261  0.341645  ...  0.194007 -0.084169 -0.293965   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "153  0.187993  0.534930  0.465906  ...  0.435125  0.082796  0.125401   \n",
       "154  0.273583  0.391672  0.145301  ...  0.274637  0.080526  0.021162   \n",
       "155  0.250711  0.529989  0.315082  ...  0.481008  0.202375 -0.087569   \n",
       "156  0.192592  0.507035  0.459623  ...  0.261736  0.200122  0.139826   \n",
       "157  0.323051  0.333396  0.076793  ...  0.570339  0.149866  0.088419   \n",
       "\n",
       "        64613     64614     64615     64616     64617     64618     64619  \n",
       "0    0.057737  0.108180  0.056225  0.042079  0.084301  0.041814  0.307135  \n",
       "1    0.365236  0.236431  0.193829  0.247808  0.266242  0.295980  0.532271  \n",
       "2    0.229688  0.116440  0.049035  0.071814  0.219577  0.078079  0.439387  \n",
       "3    0.003119  0.063941  0.240621  0.062928  0.136225  0.082756  0.281293  \n",
       "4   -0.067129  0.015106 -0.066016  0.092298  0.061617  0.007905  0.311022  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "153  0.225401  0.045384  0.137809  0.239056  0.178493  0.124486  0.334965  \n",
       "154  0.204128  0.080962  0.033422  0.177067  0.244266  0.238470  0.499744  \n",
       "155 -0.172030  0.112162  0.011920 -0.065025 -0.027739  0.059129  0.414311  \n",
       "156  0.057603  0.277383  0.374788  0.009510  0.280210  0.196362  0.292409  \n",
       "157  0.147588  0.335105  0.178705  0.190511  0.100918  0.140988  0.321525  \n",
       "\n",
       "[158 rows x 64620 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[698.],\n",
       "       [623.],\n",
       "       [664.],\n",
       "       [776.],\n",
       "       [663.],\n",
       "       [620.],\n",
       "       [698.],\n",
       "       [770.],\n",
       "       [729.],\n",
       "       [732.],\n",
       "       [750.],\n",
       "       [600.],\n",
       "       [730.],\n",
       "       [667.],\n",
       "       [595.],\n",
       "       [697.],\n",
       "       [672.],\n",
       "       [699.],\n",
       "       [728.],\n",
       "       [581.],\n",
       "       [689.],\n",
       "       [650.],\n",
       "       [729.],\n",
       "       [726.],\n",
       "       [742.],\n",
       "       [695.],\n",
       "       [741.],\n",
       "       [695.],\n",
       "       [577.],\n",
       "       [600.],\n",
       "       [664.],\n",
       "       [712.],\n",
       "       [729.],\n",
       "       [734.],\n",
       "       [745.],\n",
       "       [647.],\n",
       "       [709.],\n",
       "       [670.],\n",
       "       [747.],\n",
       "       [671.],\n",
       "       [596.],\n",
       "       [738.],\n",
       "       [786.],\n",
       "       [706.],\n",
       "       [689.],\n",
       "       [670.],\n",
       "       [780.],\n",
       "       [698.],\n",
       "       [663.],\n",
       "       [734.],\n",
       "       [667.],\n",
       "       [723.],\n",
       "       [768.],\n",
       "       [750.],\n",
       "       [740.],\n",
       "       [738.],\n",
       "       [734.],\n",
       "       [729.],\n",
       "       [745.],\n",
       "       [585.],\n",
       "       [692.],\n",
       "       [669.],\n",
       "       [737.],\n",
       "       [711.],\n",
       "       [730.],\n",
       "       [728.],\n",
       "       [641.],\n",
       "       [698.],\n",
       "       [763.],\n",
       "       [697.],\n",
       "       [685.],\n",
       "       [718.],\n",
       "       [678.],\n",
       "       [730.],\n",
       "       [633.],\n",
       "       [640.],\n",
       "       [680.],\n",
       "       [734.],\n",
       "       [752.],\n",
       "       [693.],\n",
       "       [683.],\n",
       "       [709.],\n",
       "       [728.],\n",
       "       [763.],\n",
       "       [710.],\n",
       "       [678.],\n",
       "       [690.],\n",
       "       [677.],\n",
       "       [719.],\n",
       "       [669.],\n",
       "       [730.],\n",
       "       [732.],\n",
       "       [710.],\n",
       "       [695.],\n",
       "       [756.],\n",
       "       [703.],\n",
       "       [646.],\n",
       "       [700.],\n",
       "       [727.],\n",
       "       [681.],\n",
       "       [687.],\n",
       "       [685.],\n",
       "       [759.],\n",
       "       [625.],\n",
       "       [732.],\n",
       "       [720.],\n",
       "       [624.],\n",
       "       [661.],\n",
       "       [776.],\n",
       "       [747.],\n",
       "       [756.],\n",
       "       [725.],\n",
       "       [640.],\n",
       "       [719.],\n",
       "       [680.],\n",
       "       [680.],\n",
       "       [734.],\n",
       "       [723.],\n",
       "       [708.],\n",
       "       [726.],\n",
       "       [714.],\n",
       "       [711.],\n",
       "       [657.],\n",
       "       [649.],\n",
       "       [736.],\n",
       "       [700.],\n",
       "       [682.],\n",
       "       [540.],\n",
       "       [642.],\n",
       "       [603.],\n",
       "       [655.],\n",
       "       [622.],\n",
       "       [670.],\n",
       "       [543.],\n",
       "       [591.],\n",
       "       [792.],\n",
       "       [668.],\n",
       "       [634.],\n",
       "       [656.],\n",
       "       [447.],\n",
       "       [633.],\n",
       "       [657.],\n",
       "       [650.],\n",
       "       [518.],\n",
       "       [581.],\n",
       "       [635.],\n",
       "       [573.],\n",
       "       [625.],\n",
       "       [604.],\n",
       "       [614.],\n",
       "       [664.],\n",
       "       [630.],\n",
       "       [617.],\n",
       "       [547.],\n",
       "       [672.],\n",
       "       [528.],\n",
       "       [540.],\n",
       "       [640.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149.],\n",
       "       [133.],\n",
       "       [134.],\n",
       "       [143.],\n",
       "       [108.],\n",
       "       [110.],\n",
       "       [145.],\n",
       "       [150.],\n",
       "       [135.],\n",
       "       [140.],\n",
       "       [142.],\n",
       "       [120.],\n",
       "       [141.],\n",
       "       [137.],\n",
       "       [106.],\n",
       "       [138.],\n",
       "       [113.],\n",
       "       [138.],\n",
       "       [148.],\n",
       "       [117.],\n",
       "       [145.],\n",
       "       [113.],\n",
       "       [139.],\n",
       "       [130.],\n",
       "       [144.],\n",
       "       [140.],\n",
       "       [138.],\n",
       "       [145.],\n",
       "       [117.],\n",
       "       [ 90.],\n",
       "       [110.],\n",
       "       [129.],\n",
       "       [139.],\n",
       "       [130.],\n",
       "       [145.],\n",
       "       [112.],\n",
       "       [138.],\n",
       "       [132.],\n",
       "       [136.],\n",
       "       [124.],\n",
       "       [110.],\n",
       "       [150.],\n",
       "       [148.],\n",
       "       [138.],\n",
       "       [148.],\n",
       "       [148.],\n",
       "       [150.],\n",
       "       [147.],\n",
       "       [132.],\n",
       "       [145.],\n",
       "       [130.],\n",
       "       [137.],\n",
       "       [148.],\n",
       "       [142.],\n",
       "       [149.],\n",
       "       [140.],\n",
       "       [138.],\n",
       "       [141.],\n",
       "       [135.],\n",
       "       [117.],\n",
       "       [136.],\n",
       "       [125.],\n",
       "       [146.],\n",
       "       [137.],\n",
       "       [150.],\n",
       "       [132.],\n",
       "       [ 99.],\n",
       "       [114.],\n",
       "       [147.],\n",
       "       [141.],\n",
       "       [130.],\n",
       "       [145.],\n",
       "       [124.],\n",
       "       [140.],\n",
       "       [125.],\n",
       "       [150.],\n",
       "       [138.],\n",
       "       [142.],\n",
       "       [146.],\n",
       "       [139.],\n",
       "       [147.],\n",
       "       [128.],\n",
       "       [142.],\n",
       "       [148.],\n",
       "       [148.],\n",
       "       [131.],\n",
       "       [141.],\n",
       "       [131.],\n",
       "       [136.],\n",
       "       [134.],\n",
       "       [143.],\n",
       "       [150.],\n",
       "       [138.],\n",
       "       [122.],\n",
       "       [143.],\n",
       "       [131.],\n",
       "       [124.],\n",
       "       [143.],\n",
       "       [132.],\n",
       "       [132.],\n",
       "       [135.],\n",
       "       [127.],\n",
       "       [147.],\n",
       "       [113.],\n",
       "       [150.],\n",
       "       [136.],\n",
       "       [134.],\n",
       "       [131.],\n",
       "       [148.],\n",
       "       [142.],\n",
       "       [148.],\n",
       "       [136.],\n",
       "       [119.],\n",
       "       [140.],\n",
       "       [133.],\n",
       "       [140.],\n",
       "       [138.],\n",
       "       [142.],\n",
       "       [141.],\n",
       "       [135.],\n",
       "       [126.],\n",
       "       [145.],\n",
       "       [135.],\n",
       "       [114.],\n",
       "       [141.],\n",
       "       [135.],\n",
       "       [146.],\n",
       "       [ 99.],\n",
       "       [140.],\n",
       "       [140.],\n",
       "       [136.],\n",
       "       [143.],\n",
       "       [135.],\n",
       "       [111.],\n",
       "       [120.],\n",
       "       [149.],\n",
       "       [134.],\n",
       "       [118.],\n",
       "       [124.],\n",
       "       [ 90.],\n",
       "       [129.],\n",
       "       [120.],\n",
       "       [117.],\n",
       "       [113.],\n",
       "       [120.],\n",
       "       [130.],\n",
       "       [108.],\n",
       "       [109.],\n",
       "       [127.],\n",
       "       [ 90.],\n",
       "       [134.],\n",
       "       [121.],\n",
       "       [125.],\n",
       "       [115.],\n",
       "       [138.],\n",
       "       [ 97.],\n",
       "       [ 99.],\n",
       "       [ 90.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124.],\n",
       "       [121.],\n",
       "       [120.],\n",
       "       [148.],\n",
       "       [142.],\n",
       "       [100.],\n",
       "       [130.],\n",
       "       [135.],\n",
       "       [130.],\n",
       "       [148.],\n",
       "       [143.],\n",
       "       [111.],\n",
       "       [137.],\n",
       "       [148.],\n",
       "       [122.],\n",
       "       [129.],\n",
       "       [145.],\n",
       "       [130.],\n",
       "       [132.],\n",
       "       [105.],\n",
       "       [130.],\n",
       "       [132.],\n",
       "       [134.],\n",
       "       [150.],\n",
       "       [136.],\n",
       "       [120.],\n",
       "       [148.],\n",
       "       [116.],\n",
       "       [113.],\n",
       "       [134.],\n",
       "       [120.],\n",
       "       [145.],\n",
       "       [148.],\n",
       "       [145.],\n",
       "       [142.],\n",
       "       [129.],\n",
       "       [133.],\n",
       "       [111.],\n",
       "       [148.],\n",
       "       [131.],\n",
       "       [117.],\n",
       "       [147.],\n",
       "       [150.],\n",
       "       [130.],\n",
       "       [125.],\n",
       "       [125.],\n",
       "       [148.],\n",
       "       [134.],\n",
       "       [130.],\n",
       "       [132.],\n",
       "       [120.],\n",
       "       [144.],\n",
       "       [147.],\n",
       "       [140.],\n",
       "       [140.],\n",
       "       [140.],\n",
       "       [146.],\n",
       "       [135.],\n",
       "       [145.],\n",
       "       [116.],\n",
       "       [130.],\n",
       "       [134.],\n",
       "       [137.],\n",
       "       [139.],\n",
       "       [145.],\n",
       "       [148.],\n",
       "       [140.],\n",
       "       [146.],\n",
       "       [142.],\n",
       "       [135.],\n",
       "       [150.],\n",
       "       [140.],\n",
       "       [135.],\n",
       "       [150.],\n",
       "       [124.],\n",
       "       [145.],\n",
       "       [125.],\n",
       "       [138.],\n",
       "       [140.],\n",
       "       [131.],\n",
       "       [119.],\n",
       "       [143.],\n",
       "       [139.],\n",
       "       [141.],\n",
       "       [136.],\n",
       "       [124.],\n",
       "       [118.],\n",
       "       [127.],\n",
       "       [137.],\n",
       "       [132.],\n",
       "       [139.],\n",
       "       [133.],\n",
       "       [134.],\n",
       "       [141.],\n",
       "       [149.],\n",
       "       [135.],\n",
       "       [119.],\n",
       "       [130.],\n",
       "       [145.],\n",
       "       [137.],\n",
       "       [138.],\n",
       "       [136.],\n",
       "       [144.],\n",
       "       [128.],\n",
       "       [127.],\n",
       "       [136.],\n",
       "       [119.],\n",
       "       [122.],\n",
       "       [147.],\n",
       "       [144.],\n",
       "       [143.],\n",
       "       [147.],\n",
       "       [124.],\n",
       "       [140.],\n",
       "       [143.],\n",
       "       [126.],\n",
       "       [146.],\n",
       "       [131.],\n",
       "       [133.],\n",
       "       [145.],\n",
       "       [104.],\n",
       "       [139.],\n",
       "       [124.],\n",
       "       [128.],\n",
       "       [136.],\n",
       "       [135.],\n",
       "       [123.],\n",
       "       [120.],\n",
       "       [120.],\n",
       "       [ 90.],\n",
       "       [129.],\n",
       "       [ 91.],\n",
       "       [120.],\n",
       "       [102.],\n",
       "       [102.],\n",
       "       [150.],\n",
       "       [128.],\n",
       "       [125.],\n",
       "       [127.],\n",
       "       [ 95.],\n",
       "       [124.],\n",
       "       [130.],\n",
       "       [128.],\n",
       "       [ 87.],\n",
       "       [130.],\n",
       "       [120.],\n",
       "       [116.],\n",
       "       [123.],\n",
       "       [113.],\n",
       "       [100.],\n",
       "       [128.],\n",
       "       [125.],\n",
       "       [125.],\n",
       "       [121.],\n",
       "       [122.],\n",
       "       [107.],\n",
       "       [120.],\n",
       "       [120.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_verbal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - General Scores (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.2818141  -0.66885296 -0.68311795 -0.76021504 -1.8059709  -1.55481273\n",
      " -1.15783791 -0.61605366 -1.38966716 -0.51998405]\n",
      "R^2 Train score:  0.20795173256329347\n",
      "Test MSE: 1.47, r: 0.052, p value: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Math Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-0.58263797 -0.74666216 -1.8529241  -0.68984266 -1.55708029 -1.57655135\n",
      " -1.46703594 -0.67100396 -1.52061832 -0.65374476]\n",
      "R^2 Train score:  0.23422633906679868\n",
      "Test MSE: 1.009, r: 0.203, p value: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Verbal Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.46800768 -1.14368137 -1.36455553 -0.99748263 -1.62702485 -0.7226486\n",
      " -1.27306569 -0.67546218 -0.8628129  -0.78450683]\n",
      "R^2 Train score:  0.15972297734021657\n",
      "Test MSE: 1.146, r: -0.011, p value: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge - General scores for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.28180944 -0.66885238 -0.68312027 -0.76021391 -1.80595279 -1.55480942\n",
      " -1.15783356 -0.6160493  -1.38966296 -0.51998159]\n",
      "R^2 Train score:  0.20795173254783617\n",
      "Test MSE: 1.47, r: 0.052, p value: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "rdg = Ridge()\n",
    "rdg.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(rdg, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", rdg.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = rdg.predict(X_test_pca)\n",
    "test_mse_rdg = mean_squared_error(y_test, y_test_predicted)\n",
    "r_rdg, p_val_rdg = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_rdg, 3)}, r: {np.round(r_rdg, 3)}, p value: {np.round(p_val_rdg, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge - Math scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-0.58263711 -0.74666334 -1.85291591 -0.68983986 -1.55705008 -1.57653893\n",
      " -1.46702418 -0.6709899  -1.52061445 -0.65374161]\n",
      "R^2 Train score:  0.23422633904858192\n",
      "Test MSE: 1.009, r: 0.203, p value: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "rdg = Ridge()\n",
    "rdg.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(rdg, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", rdg.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = rdg.predict(X_test_pca)\n",
    "test_mse_rdg = mean_squared_error(y_test, y_test_predicted)\n",
    "r_rdg, p_val_rdg = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_rdg, 3)}, r: {np.round(r_rdg, 3)}, p value: {np.round(p_val_rdg, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge - verbal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.46800586 -1.14367927 -1.36455801 -0.99748039 -1.62701703 -0.72264559\n",
      " -1.27306207 -0.67546051 -0.86280561 -0.7845033 ]\n",
      "R^2 Train score:  0.15972297733059382\n",
      "Test MSE: 1.146, r: -0.011, p value: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "rdg = Ridge()\n",
    "rdg.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(rdg, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", rdg.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = rdg.predict(X_test_pca)\n",
    "test_mse_rdg = mean_squared_error(y_test, y_test_predicted)\n",
    "r_rdg, p_val_rdg = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_rdg, 3)}, r: {np.round(r_rdg, 3)}, p value: {np.round(p_val_rdg, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso - general scores for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.19800335 -0.7225765  -0.78912827 -0.78396773 -1.6252855  -1.59285778\n",
      " -1.10947967 -0.57467225 -1.34629703 -0.47906367]\n",
      "R^2 Train score:  0.18565988431273794\n",
      "Test MSE: 1.399, r: 0.191, p value: 0.479\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = Lasso()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso - Math scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-0.57239194 -0.77684478 -1.56965015 -0.68293803 -1.2937692  -1.35428023\n",
      " -1.30296963 -0.52204149 -1.46662509 -0.59886447]\n",
      "R^2 Train score:  0.21322538272539815\n",
      "Test MSE: 0.991, r: 0.202, p value: 0.452\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = Lasso()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso - verbal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.42978978 -1.12452779 -1.51747579 -0.92974648 -1.51098359 -0.69585245\n",
      " -1.2300397  -0.67009528 -0.75607143 -0.68348459]\n",
      "R^2 Train score:  0.13864232204897353\n",
      "Test MSE: 1.072, r: 0.072, p value: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = Lasso()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net - general scores for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.22209041 -0.69626841 -0.72218165 -0.77647268 -1.67728205 -1.56009265\n",
      " -1.11011769 -0.58946189 -1.35776575 -0.49884928]\n",
      "R^2 Train score:  0.20187697135245597\n",
      "Test MSE: 1.428, r: 0.127, p value: 0.639\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = ElasticNet()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net - math scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-0.57941432 -0.75198563 -1.7009691  -0.68987898 -1.35573364 -1.43736374\n",
      " -1.37803323 -0.55055475 -1.48920863 -0.6132642 ]\n",
      "R^2 Train score:  0.22840012220272354\n",
      "Test MSE: 1.003, r: 0.195, p value: 0.469\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = ElasticNet()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net - verbal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:  [-1.44590021 -1.13571277 -1.46183202 -0.9584726  -1.55976094 -0.70858743\n",
      " -1.26508302 -0.68000573 -0.80108539 -0.73866884]\n",
      "R^2 Train score:  0.15381780746615548\n",
      "Test MSE: 1.11, r: 0.021, p value: 0.939\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "lr = ElasticNet()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "cross_val_score_list = cross_val_score(lr, X_train_pca, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "print(\"cross validation score: \", cross_val_score_list)\n",
    "print(\"R^2 Train score: \", lr.score(X_train_pca, y_train))\n",
    "\n",
    "y_test_predicted = lr.predict(X_test_pca)\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_lr, p_val_lr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_lr, 3)}, r: {np.round(r_lr, 3)}, p value: {np.round(p_val_lr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - general scores for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r:  0.1296659421932517 , mean p-value:  0.4914199451208222 , mean val MSE:  1.0149050865181246\n",
      "min MSE: 0.579972475340763 in fold 4\n",
      "{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "MSE: 1.389, r: 0.235, p value: 0.381\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=75, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Parameters grid for hyperparameter tuning \n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "r_vec = []\n",
    "p_value_vec = []\n",
    "fold_mse_train = []\n",
    "fold_mse_val = []\n",
    "fold_config = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_pca):\n",
    "    X_train_fold, X_val_fold = X_train_pca[train_index], X_train_pca[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    random_forest = RandomForestRegressor(random_state=0)\n",
    "    grid_search = GridSearchCV(\n",
    "        random_forest,\n",
    "        param_grid=param_grid_rf,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Fit and Predict\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_train_predicted = best_estimator.predict(X_train_fold)\n",
    "    y_predicted = best_estimator.predict(X_val_fold)\n",
    "    fold_config.append(grid_search.best_params_)\n",
    "    train_mse, val_mse = mean_squared_error(y_train_fold, y_train_predicted), mean_squared_error(y_val_fold, y_predicted)\n",
    "\n",
    "    r, p_value = pearsonr(y_predicted, y_val_fold)\n",
    "    r_vec.append(r)\n",
    "    p_value_vec.append(p_value)\n",
    "    fold_mse_train.append(train_mse)\n",
    "    fold_mse_val.append(val_mse)\n",
    "\n",
    "print(\"mean r: \", np.mean(r_vec), \", mean p-value: \", np.mean(p_value_vec),  \", mean val MSE: \", np.mean(fold_mse_val))\n",
    "print(f\"min MSE: {np.min(fold_mse_val)} in fold {np.argmin(fold_mse_val) + 1}\")\n",
    "# Configuration that received the minimal MSE\n",
    "config = fold_config[np.argmin(fold_mse_val)]\n",
    "print(config)\n",
    "\n",
    "# testing model on test set\n",
    "chosen_random_forest = RandomForestRegressor(random_state=0, **config)\n",
    "chosen_random_forest.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = chosen_random_forest.predict(X_test_pca)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r, p_value = pearsonr(y_test, y_pred)\n",
    "print(f\"MSE: {np.round(mse, 3)}, r: {np.round(r, 3)}, p value: {np.round(p_value, 3)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - math scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r:  0.17857560160950278 , mean p-value:  0.5083428144920112 , mean val MSE:  0.991687101466925\n",
      "min MSE: 0.38100181069770295 in fold 5\n",
      "{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "MSE: 0.853, r: 0.66, p value: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=75, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Parameters grid for hyperparameter tuning \n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "r_vec = []\n",
    "p_value_vec = []\n",
    "fold_mse_train = []\n",
    "fold_mse_val = []\n",
    "fold_config = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_pca):\n",
    "    X_train_fold, X_val_fold = X_train_pca[train_index], X_train_pca[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    random_forest = RandomForestRegressor(random_state=0)\n",
    "    grid_search = GridSearchCV(\n",
    "        random_forest,\n",
    "        param_grid=param_grid_rf,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Fit and Predict\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_train_predicted = best_estimator.predict(X_train_fold)\n",
    "    y_predicted = best_estimator.predict(X_val_fold)\n",
    "    fold_config.append(grid_search.best_params_)\n",
    "    train_mse, val_mse = mean_squared_error(y_train_fold, y_train_predicted), mean_squared_error(y_val_fold, y_predicted)\n",
    "\n",
    "    r, p_value = pearsonr(y_predicted, y_val_fold)\n",
    "    r_vec.append(r)\n",
    "    p_value_vec.append(p_value)\n",
    "    fold_mse_train.append(train_mse)\n",
    "    fold_mse_val.append(val_mse)\n",
    "\n",
    "print(\"mean r: \", np.mean(r_vec), \", mean p-value: \", np.mean(p_value_vec),  \", mean val MSE: \", np.mean(fold_mse_val))\n",
    "print(f\"min MSE: {np.min(fold_mse_val)} in fold {np.argmin(fold_mse_val) + 1}\")\n",
    "# Configuration that received the minimal MSE\n",
    "config = fold_config[np.argmin(fold_mse_val)]\n",
    "print(config)\n",
    "\n",
    "# testing model on test set\n",
    "chosen_random_forest = RandomForestRegressor(random_state=0, **config)\n",
    "chosen_random_forest.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = chosen_random_forest.predict(X_test_pca)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r, p_value = pearsonr(y_test, y_pred)\n",
    "print(f\"MSE: {np.round(mse, 3)}, r: {np.round(r, 3)}, p value: {np.round(p_value, 3)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - verbal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r:  0.0893067753039939 , mean p-value:  0.5900633083821171 , mean val MSE:  1.03316644759123\n",
      "min MSE: 0.47022371930182943 in fold 5\n",
      "{'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "MSE: 1.054, r: 0.33, p value: 0.212\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=75, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Parameters grid for hyperparameter tuning \n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "r_vec = []\n",
    "p_value_vec = []\n",
    "fold_mse_train = []\n",
    "fold_mse_val = []\n",
    "fold_config = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_pca):\n",
    "    X_train_fold, X_val_fold = X_train_pca[train_index], X_train_pca[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    random_forest = RandomForestRegressor(random_state=0)\n",
    "    grid_search = GridSearchCV(\n",
    "        random_forest,\n",
    "        param_grid=param_grid_rf,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Fit and Predict\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_train_predicted = best_estimator.predict(X_train_fold)\n",
    "    y_predicted = best_estimator.predict(X_val_fold)\n",
    "    fold_config.append(grid_search.best_params_)\n",
    "    train_mse, val_mse = mean_squared_error(y_train_fold, y_train_predicted), mean_squared_error(y_val_fold, y_predicted)\n",
    "\n",
    "    r, p_value = pearsonr(y_predicted, y_val_fold)\n",
    "    r_vec.append(r)\n",
    "    p_value_vec.append(p_value)\n",
    "    fold_mse_train.append(train_mse)\n",
    "    fold_mse_val.append(val_mse)\n",
    "\n",
    "print(\"mean r: \", np.mean(r_vec), \", mean p-value: \", np.mean(p_value_vec),  \", mean val MSE: \", np.mean(fold_mse_val))\n",
    "print(f\"min MSE: {np.min(fold_mse_val)} in fold {np.argmin(fold_mse_val) + 1}\")\n",
    "# Configuration that received the minimal MSE\n",
    "config = fold_config[np.argmin(fold_mse_val)]\n",
    "print(config)\n",
    "\n",
    "# testing model on test set\n",
    "chosen_random_forest = RandomForestRegressor(random_state=0, **config)\n",
    "chosen_random_forest.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = chosen_random_forest.predict(X_test_pca)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r, p_value = pearsonr(y_test, y_pred)\n",
    "print(f\"MSE: {np.round(mse, 3)}, r: {np.round(r, 3)}, p value: {np.round(p_value, 3)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting - general scores for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.358, r: 0.299, p value: 0.26\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=0)\n",
    "gbr.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted = gbr.predict(X_test_pca)\n",
    "test_mse_gbr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_gbr, p_val_gbr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_gbr, 3)}, r: {np.round(r_gbr, 3)}, p value: {np.round(p_val_gbr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting - math scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.903, r: 0.309, p value: 0.244\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_math, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=0)\n",
    "gbr.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted = gbr.predict(X_test_pca)\n",
    "test_mse_gbr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_gbr, p_val_gbr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_gbr, 3)}, r: {np.round(r_gbr, 3)}, p value: {np.round(p_val_gbr, 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting - verbal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.262, r: -0.203, p value: 0.451\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_verbal, test_size=0.1, random_state=0)\n",
    "\n",
    "# Normalization of features and behavioral scores\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train).ravel()\n",
    "y_test = scaler.transform(y_test).ravel()\n",
    "\n",
    "# Apply PCA for feature selection\n",
    "pca = PCA(n_components=20, svd_solver=\"full\", random_state=0)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "# Apply PCA on the testing data\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=0)\n",
    "gbr.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted = gbr.predict(X_test_pca)\n",
    "test_mse_gbr = mean_squared_error(y_test, y_test_predicted)\n",
    "r_gbr, p_val_gbr = pearsonr(y_test, y_test_predicted)\n",
    "print(f\"Test MSE: {np.round(test_mse_gbr, 3)}, r: {np.round(r_gbr, 3)}, p value: {np.round(p_val_gbr, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
